{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b487588-9b24-4cb0-af3c-b7d1cbb83dd9",
   "metadata": {},
   "source": [
    "Airline Delay and Cancellation Analysis (2009-2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dde65-cce7-436f-a3ac-4e398ab1d089",
   "metadata": {},
   "source": [
    "This project aims to analyze a comprehensive dataset of airline delays and cancellations spanning from 2009 to 2018. We will explore various factors contributing to flight disruptions, identify patterns across years, and potentially build predictive models to enhance understanding and operational efficiency within the aviation industry.\n",
    "\n",
    "Project Goal:\n",
    "\n",
    "The primary goal of this project is to gain actionable insights from historical flight data to understand the root causes of delays and cancellations over a decade. By doing so, we aim to provide valuable information for airlines to improve their on-time performance, for airports to optimize operations, and for passengers to make informed travel decisions. Ultimately, we seek to develop a robust data analysis and machine learning pipeline that showcases advanced data science techniques, handling multi-file datasets effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c330d0-654c-4534-9864-beffac8215ac",
   "metadata": {},
   "source": [
    "1. Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906df8eb-cb05-46f4-92d0-6a4d938d5489",
   "metadata": {},
   "source": [
    "This initial section sets up our Python environment by importing necessary libraries and loading the raw flight data from multiple yearly CSV files into a single, combined Pandas DataFrame.\n",
    "\n",
    "    Simple Explanation\n",
    "\n",
    "To manage the large dataset spread across multiple yearly files, we'll first import essential Python libraries like Pandas for data manipulation and NumPy for numerical operations. We'll then iterate through each year's file (from 2009 to 2018), load it individually, and combine all these yearly datasets into one large, unified DataFrame. This is crucial for analyzing trends and patterns over the entire decade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5421621a-36df-4141-a9eb-a5584cfbe53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HB Laptop Store\\Desktop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30235942-5841-4b6d-b886-7457b5335bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory changed to: C:\\Users\\HB Laptop Store\\Desktop\\Airline_delay_cancellation\n",
      "\n",
      "Loading data with selected columns to optimize memory usage...\n",
      "Successfully loaded file: 2009.csv with selected columns (16 columns).\n",
      "Successfully loaded file: 2010.csv with selected columns (16 columns).\n",
      "Successfully loaded file: 2011.csv with selected columns (16 columns).\n",
      "Successfully loaded file: 2012.csv with selected columns (16 columns).\n",
      "Successfully loaded file: 2013.csv with selected columns (16 columns).\n",
      "Successfully loaded file: 2014.csv with selected columns (16 columns).\n",
      "Successfully loaded file: 2015.csv with selected columns (16 columns).\n",
      "Successfully loaded file: 2016.csv with selected columns (16 columns).\n",
      "Unexpected error loading file 2017.csv: Unable to allocate 433. MiB for an array with shape (10, 5674621) and data type float64\n",
      "Unexpected error loading file 2018.csv: Unable to allocate 550. MiB for an array with shape (10, 7213446) and data type float64\n",
      "Error concatenating data: Unable to allocate 1.09 GiB for an array with shape (3, 48668897) and data type float64. If this is still a MemoryError, consider reducing the number of columns further or processing data in chunks for analysis.\n",
      "\n",
      "Code execution finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Define the correct path to your data files\n",
    "# This path was successfully identified in your previous step.\n",
    "data_folder_path = r'C:\\Users\\HB Laptop Store\\Desktop\\Airline_delay_cancellation'\n",
    "\n",
    "# 2. Change the current working directory of the program to the data folder\n",
    "try:\n",
    "    os.chdir(data_folder_path)\n",
    "    print(f\"Current working directory changed to: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The folder {data_folder_path} was not found. Please check the path and rerun.\")\n",
    "    exit() # Exit if the folder is not found\n",
    "\n",
    "# 3. List to store DataFrames from each CSV file\n",
    "all_data = []\n",
    "\n",
    "# 4. Define the range of years to read data for\n",
    "start_year = 2009\n",
    "end_year = 2018\n",
    "\n",
    "# ***** SELECTED COLUMNS FOR ANALYSIS *****\n",
    "# These columns are chosen based on common airline delay/cancellation analysis.\n",
    "columns_to_use = [\n",
    "    'FL_DATE',\n",
    "    'OP_CARRIER',\n",
    "    'OP_CARRIER_FL_NUM',\n",
    "    'ORIGIN',\n",
    "    'DEST',\n",
    "    'DEP_DELAY',\n",
    "    'ARR_DELAY',\n",
    "    'CANCELLED',\n",
    "    'CANCELLATION_CODE',\n",
    "    'DIVERTED',\n",
    "    'DISTANCE',\n",
    "    'CARRIER_DELAY',\n",
    "    'WEATHER_DELAY',\n",
    "    'NAS_DELAY',\n",
    "    'SECURITY_DELAY',\n",
    "    'LATE_AIRCRAFT_DELAY'\n",
    "]\n",
    "# ****************************************\n",
    "\n",
    "print(\"\\nLoading data with selected columns to optimize memory usage...\")\n",
    "\n",
    "# 5. Loop through each year to read the corresponding CSV file\n",
    "for year in range(start_year, end_year + 1):\n",
    "    file_name = f'{year}.csv'\n",
    "    try:\n",
    "        # Read the CSV file, specifying only the selected columns using 'usecols'\n",
    "        # This is the primary method to save memory when dealing with large files.\n",
    "        df_year = pd.read_csv(file_name, usecols=columns_to_use)\n",
    "        all_data.append(df_year)\n",
    "        print(f\"Successfully loaded file: {file_name} with selected columns ({len(columns_to_use)} columns).\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File {file_name} not found. Skipping this year.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Warning: File {file_name} is empty. Skipping this year.\")\n",
    "    except ValueError as ve:\n",
    "        # This error can occur if a specified column is not found in a particular CSV file.\n",
    "        print(f\"Error loading {file_name} (ValueError): {ve}. This might mean some selected columns are missing in this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error loading file {file_name}: {e}\")\n",
    "\n",
    "# 6. Concatenate all loaded DataFrames into one main DataFrame\n",
    "if all_data:\n",
    "    try:\n",
    "        main_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(\"\\nAll data successfully concatenated.\")\n",
    "        print(f\"Final DataFrame shape: {main_df.shape[0]} rows and {main_df.shape[1]} columns.\")\n",
    "        \n",
    "        # Display the first few rows and memory usage for verification\n",
    "        print(\"\\nFirst 5 rows of the combined DataFrame:\")\n",
    "        print(main_df.head())\n",
    "        print(f\"\\nTotal memory usage of the DataFrame: {main_df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating data: {e}. If this is still a MemoryError, consider reducing the number of columns further or processing data in chunks for analysis.\")\n",
    "else:\n",
    "    print(\"\\nNo data files were loaded successfully. Please ensure files are correctly extracted and try again.\")\n",
    "\n",
    "print(\"\\nCode execution finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bc87a-651e-4f8a-92b5-bad65708887b",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70265b-9a34-4daa-bd67-ecbd1d107f7c",
   "metadata": {},
   "source": [
    "2. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068959ba-b7f8-4243-94ec-f91152df1bac",
   "metadata": {},
   "source": [
    "Once the combined data is loaded, it's crucial to perform an initial inspection to understand its structure, content, and identify any immediate issues across the entire dataset.\n",
    "\n",
    "Simple Explanation\n",
    "\n",
    "Now that we have all the yearly data combined, it's time to take a quick look at the complete dataset. We'll check the first few rows, get a summary of data types, see how many non-null values each column has, and generate basic statistics. This comprehensive overview helps us understand the full scope of our data and pinpoint any problems (like missing values or incorrect data types) that need fixing before we proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4db51e7-8a48-40af-b875-4daabbe9a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initial Data Inspection ---\n",
      "\n",
      "1. First 5 rows of the combined DataFrame:\n",
      "      FL_DATE OP_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  DEP_DELAY  ARR_DELAY  \\\n",
      "0  2009-01-01         XE               1204    DCA  EWR       -2.0        4.0   \n",
      "1  2009-01-01         XE               1206    EWR  IAD       -1.0       -8.0   \n",
      "2  2009-01-01         XE               1207    EWR  DCA       -1.0       -9.0   \n",
      "3  2009-01-01         XE               1208    DCA  EWR        9.0      -12.0   \n",
      "4  2009-01-01         XE               1209    IAD  EWR      -10.0      -38.0   \n",
      "\n",
      "   CANCELLED CANCELLATION_CODE  DIVERTED  DISTANCE  CARRIER_DELAY  \\\n",
      "0        0.0               NaN       0.0     199.0            NaN   \n",
      "1        0.0               NaN       0.0     213.0            NaN   \n",
      "2        0.0               NaN       0.0     199.0            NaN   \n",
      "3        0.0               NaN       0.0     199.0            NaN   \n",
      "4        0.0               NaN       0.0     213.0            NaN   \n",
      "\n",
      "   WEATHER_DELAY  NAS_DELAY  SECURITY_DELAY  LATE_AIRCRAFT_DELAY  \n",
      "0            NaN        NaN             NaN                  NaN  \n",
      "1            NaN        NaN             NaN                  NaN  \n",
      "2            NaN        NaN             NaN                  NaN  \n",
      "3            NaN        NaN             NaN                  NaN  \n",
      "4            NaN        NaN             NaN                  NaN  \n",
      "\n",
      "2. DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61556964 entries, 0 to 61556963\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Dtype  \n",
      "---  ------               -----  \n",
      " 0   FL_DATE              object \n",
      " 1   OP_CARRIER           object \n",
      " 2   OP_CARRIER_FL_NUM    int64  \n",
      " 3   ORIGIN               object \n",
      " 4   DEST                 object \n",
      " 5   DEP_DELAY            float64\n",
      " 6   ARR_DELAY            float64\n",
      " 7   CANCELLED            float64\n",
      " 8   CANCELLATION_CODE    object \n",
      " 9   DIVERTED             float64\n",
      " 10  DISTANCE             float64\n",
      " 11  CARRIER_DELAY        float64\n",
      " 12  WEATHER_DELAY        float64\n",
      " 13  NAS_DELAY            float64\n",
      " 14  SECURITY_DELAY       float64\n",
      " 15  LATE_AIRCRAFT_DELAY  float64\n",
      "dtypes: float64(10), int64(1), object(5)\n",
      "memory usage: 7.3+ GB\n",
      "\n",
      "3. Descriptive Statistics for Numerical Columns:\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.38 GiB for an array with shape (3, 61556964) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 3. Display descriptive statistics for numerical columns\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. Descriptive Statistics for Numerical Columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(main_df\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 4. Check for missing values (NaN) in each column\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4. Missing Values per Column:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:11976\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[1;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[0;32m  11734\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  11735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdescribe\u001b[39m(\n\u001b[0;32m  11736\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11739\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  11740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m  11741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  11742\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[0;32m  11743\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11974\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[0;32m  11975\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 11976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m describe_ndframe(\n\u001b[0;32m  11977\u001b[0m         obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11978\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m  11979\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m  11980\u001b[0m         percentiles\u001b[38;5;241m=\u001b[39mpercentiles,\n\u001b[0;32m  11981\u001b[0m     )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:97\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     describer \u001b[38;5;241m=\u001b[39m DataFrameDescriber(\n\u001b[0;32m     92\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[0;32m     93\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m     94\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:167\u001b[0m, in \u001b[0;36mDataFrameDescriber.describe\u001b[1;34m(self, percentiles)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdescribe\u001b[39m(\u001b[38;5;28mself\u001b[39m, percentiles: Sequence[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 167\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_data()\n\u001b[0;32m    169\u001b[0m     ldesc: \u001b[38;5;28mlist\u001b[39m[Series] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, series \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:188\u001b[0m, in \u001b[0;36mDataFrameDescriber._select_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# when some numerics are found, keep only numerics\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     default_include: \u001b[38;5;28mlist\u001b[39m[npt\u001b[38;5;241m.\u001b[39mDTypeLike] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mnumber, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 188\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39mdefault_include)\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    190\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5091\u001b[0m, in \u001b[0;36mDataFrame.select_dtypes\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   5087\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   5089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 5091\u001b[0m mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39m_get_data_subset(predicate)\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   5092\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"DataFrame\", expected \"Self\")\u001b[39;00m\n\u001b[0;32m   5093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m, deep\u001b[38;5;241m=\u001b[39mdeep)\n\u001b[0;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:796\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    794\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 796\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    797\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.38 GiB for an array with shape (3, 61556964) and data type float64"
     ]
    }
   ],
   "source": [
    "# --- 2. Initial Data Inspection - Part 1: First 5 Rows ---\n",
    "print(\"\\n--- Initial Data Inspection - Part 1: First 5 Rows ---\")\n",
    "print(\"This step gives a quick look at the data's structure and content.\")\n",
    "print(main_df.head())\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f83d7-fc76-4b72-98f6-467f9bd610ff",
   "metadata": {},
   "source": [
    "3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7a5dd-47f2-4308-a83c-759d0f50213f",
   "metadata": {},
   "source": [
    "Data cleaning is a critical phase where we address inconsistencies, missing values, and incorrect data types across the entire combined dataset to prepare it for accurate analysis.\n",
    "\n",
    "Simple Explanation\n",
    "\n",
    "Now that all our yearly data is together, we'll clean it thoroughly. This involves handling missing values by either filling them in (e.g., assuming a zero delay if no delay is recorded) or removing rows/columns that are too incomplete. We'll also ensure that all columns, especially dates and numerical values, have the correct data types. This rigorous cleaning process is vital to avoid errors in our analysis and ensure the reliability of our insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f31f6-a30b-4f1a-bd58-bae0fc56e12b",
   "metadata": {},
   "source": [
    "# Identify and quantify missing values across the entire combined DataFrame\n",
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_info_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values_count,\n",
    "    'Missing Percentage (%)': missing_values_percentage\n",
    "})\n",
    "missing_info_df = missing_info_df[missing_info_df['Missing Count'] > 0].sort_values(by='Missing Percentage (%)', ascending=False)\n",
    "\n",
    "print(\"--- Missing values summary before cleaning (across all years) ---\")\n",
    "print(missing_info_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Strategy for handling missing values in delay-related columns:\n",
    "# Fill NaN values with 0. This assumes that if a delay type is not recorded, it means that specific delay did not occur.\n",
    "delay_columns_to_fill = [\n",
    "    'ArrDelay', 'DepDelay', 'CarrierDelay', 'WeatherDelay',\n",
    "    'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'ActualElapsedTime',\n",
    "    'AirTime', 'TaxiIn', 'TaxiOut' # These might also have NaNs and can be filled with 0 or mean/median\n",
    "]\n",
    "\n",
    "print(\"--- Handling missing values in specific columns ---\")\n",
    "for col in delay_columns_to_fill:\n",
    "    if col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0) # Convert to numeric first, then fill NaNs\n",
    "            print(f\"Filled NaN values in '{col}' with 0 and ensured numeric type.\")\n",
    "        else:\n",
    "            print(f\"No missing values found in '{col}'.\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' not found in the DataFrame. Skipping.\")\n",
    "\n",
    "# Handle CancellationReason (if present, often NaN for non-cancelled flights)\n",
    "if 'CancellationReason' in df.columns:\n",
    "    if df['CancellationReason'].isnull().any():\n",
    "        df['CancellationReason'] = df['CancellationReason'].fillna('Not Cancelled')\n",
    "        print(\"Filled NaN values in 'CancellationReason' with 'Not Cancelled'.\")\n",
    "\n",
    "# Drop columns with extremely high percentages of missing values if they are not critical\n",
    "# Example: If a column like 'DivAirportLandings' has nearly all NaNs, we might drop it.\n",
    "# This decision should be based on the missing_info_df output.\n",
    "# For demonstration, let's assume no columns need to be dropped at this stage unless explicitly very sparse.\n",
    "\n",
    "\n",
    "# Convert 'FlightDate' to datetime objects\n",
    "print(\"\\n--- Correcting data types ---\")\n",
    "if 'FlightDate' in df.columns:\n",
    "    # Use 'coerce' to turn unparseable dates into NaT (Not a Time)\n",
    "    df['FlightDate'] = pd.to_datetime(df['FlightDate'], errors='coerce')\n",
    "    # After conversion, we might have NaT values if some dates were truly unparseable.\n",
    "    # We can drop rows with NaT in FlightDate or fill, depending on data integrity needs.\n",
    "    # For now, let's just report if any were coerced to NaT.\n",
    "    if df['FlightDate'].isnull().any():\n",
    "        num_invalid_dates = df['FlightDate'].isnull().sum()\n",
    "        print(f\"Warning: {num_invalid_dates} 'FlightDate' values could not be parsed and were converted to NaT.\")\n",
    "        # Option: df.dropna(subset=['FlightDate'], inplace=True) if you want to remove these rows.\n",
    "    print(\"Converted 'FlightDate' to datetime type.\")\n",
    "else:\n",
    "    print(\"Column 'FlightDate' not found. Cannot convert to datetime.\")\n",
    "\n",
    "# Re-check missing values and data types after cleaning\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"--- Missing values summary AFTER cleaning (showing only remaining missing) ---\")\n",
    "remaining_missing_info = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "if not remaining_missing_info.empty:\n",
    "    print(remaining_missing_info)\n",
    "else:\n",
    "    print(\"No significant missing values remaining in key columns.\")\n",
    "\n",
    "print(\"\\n--- Data information AFTER cleaning (df.info()) ---\")\n",
    "df.info(verbose=True, show_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
